{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/eTYfbIRZRU2rNbBlMI3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gondore/nsdc-crafter/blob/main/Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Replay Memory**\n",
        "\n",
        "Replay memory typically stores tuples (state, action, reward, next_state) to leverage past training experience\n",
        "\n",
        "__init__ initializes the memory with max length, which is necessary for memory management\n",
        "\n",
        "__store_transition__ 'pushes' a transition in the memory."
      ],
      "metadata": {
        "id": "oKQGdIesbai5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TDffXBudD1dK"
      },
      "outputs": [],
      "source": [
        "# INPUT\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        # initialize a new deque with a fixed size as the memory.\n",
        "        # this holds the transitions/tuples.\n",
        "        # HINT: Use deque(maxlen=capacity) to ensure the memory does not grow beyond the set capacity.\n",
        "\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        # this 'pushes' tuples into memory.\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # randomly sample a batch of transitions from the memory.\n",
        "        # ensure the batch size does not exceed the length of the memory.\n",
        "        # hint: Use random.sample(self.memory, batch_size) to get a random subset.\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the current size of the memory.\n",
        "        # this is used to determine when there's enough samples to begin training\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Action Selection**\n",
        "this is the class to describe how the agent decides to exploit or explore.\n",
        "__select_action__ updates epsilon using a decay formulam and selects an action.\n",
        "exloit:\n",
        "- converting current state to a tensor formatted for the policy network\n",
        "- passing the state tensor through the policy network to get q-values for all possible actions\n",
        "- finally selecting the action with the max q-value\n",
        "input: state and step\n",
        "explore:\n",
        "- randomly select an action\n",
        "output:"
      ],
      "metadata": {
        "id": "cOkUynVXg33e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class ActionSelector:\n",
        "    def __init__(self, q_network, epsilon_start, epsilon_end, epsilon_decay):\n",
        "\n",
        "        #tracks number of actions taken\n",
        "        self.current_step = 0\n",
        "        # TODO: Initialize the parameters for epsilon greedy strategy\n",
        "        # this will be epsilon_start, epsilon_end, epsilon_decay, num_actions, policy_net, and epsilon\n",
        "        # examples below\n",
        "        self.epsilon = epsilon_start  # Starting value of epsilon\n",
        "        self.epsilon = epsilon_start        # Initialize epsilon with epsilon_start\n",
        "        self.epsilon_end = epsilon_end  # Minimum value of epsilon\n",
        "\n",
        "    # TODO: implement the epsilon-greedy action selection strategy.\n",
        "    def select_action(self, state, step):\n",
        "        self.current_step += 1\n",
        "\n",
        "        # TODO: Update the value of epsilon using the decay rate formula.\n",
        "        # new epsilon = end + (epsilon start - epsilon end) * (current step / epsilon decay)**(-1)\n",
        "        self.epsilon =\n",
        "\n",
        "        # If the sample is greater than epsilon, choose the best action (exploit).\n",
        "        if np.random.rand() > self.epsilon:\n",
        "          state = #convert state to tensor and ensure data type float 32\n",
        "          with torch.no_grad():\n",
        "              state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state to tensor and add batch dimension\n",
        "              action_values = self.q_network(state_tensor)  # Get action values from the Q-network\n",
        "              action = torch.argmax(action_values).item()  # Choose the action with the highest value\n",
        "              q_values = self.policy_net(state) # this updates the qvalues\n",
        "              return q_values.max(1)[1].view(1, 1).item()\n",
        "        else:\n",
        "            # TODO: else select a random action (explore) by returning a number to represent the action\n",
        "            action =\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "ZBN3KzHiLZTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uig4CMZBk6fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN**\n",
        "\n",
        "- takes preprocessed image sequneces as input and outputs the q values for each possible action."
      ],
      "metadata": {
        "id": "yYLgEu7Hk7CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(CNN, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        # Define your convolutional layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement the forward pass\n",
        "        return x\n",
        "\n",
        "    def preprocess(self, raw_state):\n",
        "        # Preprocess the raw state (image) received from the environment\n",
        "        return raw_state\n"
      ],
      "metadata": {
        "id": "iB50nbrzLezC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **main**\n",
        "- this will import everything and start training"
      ],
      "metadata": {
        "id": "2uQgkfJAovEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import gym\n",
        "import crafter\n",
        "from ReplayMemory import ReplayMemory\n",
        "from ActionSelector import ActionSelector\n",
        "from CNN import CNN\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "NUM_EPISODES = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "MEMORY_SIZE = 10000\n",
        "\n",
        "# Environment setup\n",
        "env = crafter.env\n",
        "env.seed(42)\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Get screen size from gym observation space\n",
        "_, _, screen_height, screen_width = env.observation_space.shape\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Neural network and optimizer setup\n",
        "policy_net = CNN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = CNN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()  # Set the target net to evaluation mode\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "action_selector = ActionSelector(policy_net, n_actions, EPS_START, EPS_END, EPS_DECAY)\n",
        "\n",
        "# Transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((screen_height, screen_width)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Training loop\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = tuple(zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                            batch[3])), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch[3] if s is not None]).to(device)\n",
        "\n",
        "    state_batch = torch.cat(batch[0]).to(device)\n",
        "    action_batch = torch.cat(batch[1]).to(device)\n",
        "    reward_batch = torch.cat(batch[2]).to(device)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "for episode in range(NUM_EPISODES):\n",
        "    # Initialize the environment and state\n",
        "    state = env.reset()\n",
        "    state = transform(state).unsqueeze(0).to(device)\n",
        "\n",
        "    for t in range(1000):  # run for a maximum of 1000 steps\n",
        "        # Select and perform an action\n",
        "        action = action_selector.select_action(state, device)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        if not done:\n",
        "            next_state = transform(next_state).unsqueeze(0).to(device)\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, torch.tensor([[action]], device=device), reward, next_state)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.close()\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(policy_net.state_dict(), 'crafter_dqn.pth')\n"
      ],
      "metadata": {
        "id": "eTZLybLVLm3g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "75d67e34-93bb-4658-f96c-194d0f10670a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'crafter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c7c2363f086c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcrafter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mReplayMemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mActionSelector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActionSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'crafter'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZL0K5A7qkTS9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}